Experimental agents
===

Agents and the context they work in
----
This directory contains a set of experiments, that should eventually
lead to a framework, a very simple one, for making analytic agents.

Analytic agents are in this context defined as processes, either
fully automatic or partly manual, that takes data about user
behavoir and demographics, and translates this into offers
that can be presented to subscribers to be acted on.

     demographics+behavior -> agent -> offer to subscriber.

It is important to note that even though it does make sense
for agents to run only once, it makes even more sense if they
run more than once, meaning that future runs of agents will
be observing the results of previous actions.  This makes
puts the agents in a position to be learning agents.

Interaction with analytics systems
----
There are already many analytics systems in this world, google
analytics, firebase analytics, kissmetrics, ... . Our analytics agent
subsystem should emphaticly _not_ replicate much or anything
of what these existing systems do.   Reports, ad-hoc queries,
dashboards, setting up funnels, tracking campaigns etc.  They all do
this much better than we can hope for, so  we should use them, not
copy them.   What we _do_ want is to be able to work in concert with
them.   We want to be able to pick up events that are generated on the
basis of offers generated by our analytics agents, and track them
using external analytics tools, and to use the same terms as the
tools we are working in concert with.  This means that we need
to solve a coordination problem with respect to whatever analytics
system we choose to use.   Our current working hypothesis is that we
will be using Firebase Analytics, 


Current overall design
---


### Consumption cycle

The current overall design is what we think today should be the
architecture to aim for.    This is a moving target, tomorrow
it could be something else.  With that caveat in mind, here goes:

We set up a processing cycle with these steps and formats.
 
 * Continously collect data using whatever means necessary, and dump
 them into bigquery.  Data is stored pseudoanonymized.  The bigquery dumps should contain at least:
 ** Data consumption data.
 **  Behavior data fra analytics.
 * Periodically, every hour or so, run a kubernetes cron job that does this:
 **  Run predefined queries from the data stored in bigquery, extract tables for demoraphics, etc. that makes sense for the analytics agents to work with.  Not too much, not too little, subject to change and the models must allow for this (e.g. require that columns must be allowed to appear without consumers breaking).  Put the results into temporary bigquery tables.
 ** Translate from pseudoanonymized data into consistent pseudoanonymized datasets.

** Dump the translated & consistent dataset into one or more cloud
   storage buckets, for consumption by agents.

** Agents can then either
*** Read the data directly from the bucket programmatically
*** Read the data form the bucket via the web inteface, and then
process it from local file storage.
*** Use FUSE filesystem to read data from bucket, but present it
as a local file system.

 * Agents will then take the input, process it, produce instructions for offers to be made. Do this in the form of yaml files (or perhaps a single yaml file, see sample format in this directory).  The output is written back into a cloud storage bucket.
 * The output from the agent is then picked up by a job listening for input into buckets (either by running periodically, or by active listening for changes).  The offers are then translated into internal format offers, written into the appropriate databases, possibly signalled through messaging services and immediately picked up by the subscribers.

This completes the cycle.
 
 ### Components

A kubernetes program running either continously, or as a kubernets
batch job that exports and imports datasets via google cloud storage.
Written in Kotlin, running as a dropwizard job.

The component should use a pubsub channel to listen to changes in the
actual changes in the bucket. The reason for favoring pubsub, is that
it is possible to run tests of the component on a workstation making
it unecessary to deploy the comonent to a production-like environment
to run it in an environment that is _almost_ production like but
runs on a workstation.


Notes
---

### Some questions about the bigquery structure:

* We should use timestamps I think, since it's simpler to work with.

* We should also considere using ingestion-time partitioned tables

   https://cloud.google.com/bigquery/docs/creating-partitioned-tables

* Most of our time-dependent queries will overlap reasonably
      sized shards (days, weeks, months), so that we don't have to
      table-scan _all_ of the potentially very large datasets we will
      accumulate.

* Don't name any field 'timestamp'. It leads to endless confusion
   with the datatype named 'timestamp', and the function called
   'timestamp'.

### Formatting guidelines

* We should decide if we will use camel case or snake case in
   offer descriptions. The current firebase/firestore structure uses
   both, and that is confusing.

### Accessing data via FUSE

* To get a FUSE mounted cloud storage on a mac, do this:
   https://cloud.google.com/storage/docs/gcs-fuse

* Download from
   https://github.com/GoogleCloudPlatform/gcsfuse/
   https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/installing.md

* Install this way

       brew install gcsfuse
       sudo ln -s /usr/local/sbin/mount_gcsfuse /sbin  # For mount(8) support

* Create certficate to make fuse authenticate correctly
Make a certificate https://console.cloud.google.com/apis/credentials
To authenticate

     export GOOGLE_APPLICATION_CREDENTIALS=/some/path/pantel-credentials.json

then

    gcsfuse rmz-test-bucket /tmp/aaas

TODO
===

* Disuss with Cecilie etc. what's really necessary.
* Write kotlin/kubernetes program that will extract data from
  misc. sources and present for agent consumption/read data from
  agents.
* Declare initial victory, but be prepared for massive rewrites when
  actual experience from using these tools become available
